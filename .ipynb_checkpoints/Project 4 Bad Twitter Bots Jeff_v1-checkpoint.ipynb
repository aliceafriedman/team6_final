{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Twitter Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from twython import Twython\n",
    "import time\n",
    "import os\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.util import bigrams\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.lm import Vocabulary\n",
    "from nltk.lm import MLE\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text: NLTK and Markov Chains\n",
    "\n",
    "Using a computer to generate natural text is an extremely useful and important part of NLP. You may want to create a bot to handle front-line customer queries on your website, or, in a manner perhaps more applicable to our corpus of tweets, you may be a certain intelligence service in the Eurasian Steppe. In this section, we will use our corpus and NLP skills we've acquired to generate our own Twitter posts.\n",
    "\n",
    "Early in Chapter 1 of our Natural Language Processing book, there's an example that shows output from the generate function of Python's Natural Language Toolkit (nltk) package. Using the Bible's Book of Genesis as its corpus, the generate() function creates text based on the corpus. This intrigued us. \n",
    "  \n",
    "Unfortunately, starting with nltk version 3, the generate function was removed due to problems with NLTK's language modeling class. Going deep into the [issues section](https://github.com/nltk/nltk/issues/736) of the nltk package on Github, as of July 2019, generate appears to be available again but requires much more data preparation. We're going to try it.\n",
    "\n",
    "In this section, we will generate tweets from our corpus in two ways:\n",
    "1. Using the language model interface module from the nltk package.\n",
    "2. Creating our own markov model to generate text.\n",
    "\n",
    "###Data Preparation (Again)\n",
    "\n",
    "Again, we start with our json files from twitter that contain a user's latest tweets along with like and retweet counts and other metadata. The two json files will be loaded as a combined pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>verified</th>\n",
       "      <th>location</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95977</th>\n",
       "      <td>maddow</td>\n",
       "      <td>True</td>\n",
       "      <td>New York, NY USA</td>\n",
       "      <td>9628336</td>\n",
       "      <td>\"Some within DHS and ICE say the president app...</td>\n",
       "      <td>3043.0</td>\n",
       "      <td>4651.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95338</th>\n",
       "      <td>wikileaks</td>\n",
       "      <td>True</td>\n",
       "      <td>Everywhere</td>\n",
       "      <td>5503497</td>\n",
       "      <td>37 MEPs call on the European Commission to pre...</td>\n",
       "      <td>445.0</td>\n",
       "      <td>638.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9849</th>\n",
       "      <td>AOC</td>\n",
       "      <td>True</td>\n",
       "      <td>Bronx + Queens, NYC</td>\n",
       "      <td>4437409</td>\n",
       "      <td>This is what the United States is doing in the...</td>\n",
       "      <td>3836.0</td>\n",
       "      <td>10345.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86051</th>\n",
       "      <td>NateSilver538</td>\n",
       "      <td>True</td>\n",
       "      <td>New York</td>\n",
       "      <td>3202795</td>\n",
       "      <td>@SeanMcElwee @DataProgress Glad to see de Blas...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>208.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22595</th>\n",
       "      <td>HEELZiggler</td>\n",
       "      <td>True</td>\n",
       "      <td>Hollywood, CA</td>\n",
       "      <td>2827306</td>\n",
       "      <td>@go_kings_go25 @PWTees ü§òüèΩü§òüèΩi saw it</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66225</th>\n",
       "      <td>verified</td>\n",
       "      <td>True</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>2692833</td>\n",
       "      <td>We've paused public submissions for verificati...</td>\n",
       "      <td>2070.0</td>\n",
       "      <td>11968.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23606</th>\n",
       "      <td>marwilliamson</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>2615633</td>\n",
       "      <td>@US395 @AlanaKStewart Noooooo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95126</th>\n",
       "      <td>jaketapper</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>2069987</td>\n",
       "      <td>Amid controversy, Biden gets support from high...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16955</th>\n",
       "      <td>AlaattinCAGIL</td>\n",
       "      <td>True</td>\n",
       "      <td>London, England</td>\n",
       "      <td>1647751</td>\n",
       "      <td>RT @AlaattinCAGIL: 4 boyutlu tarayƒ±cƒ± ile  Ann...</td>\n",
       "      <td>209.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22259</th>\n",
       "      <td>johncusack</td>\n",
       "      <td>True</td>\n",
       "      <td>USA</td>\n",
       "      <td>1623173</td>\n",
       "      <td>Cubs Mets https://t.co/Q6NdtKqBwv</td>\n",
       "      <td>6.0</td>\n",
       "      <td>146.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58768</th>\n",
       "      <td>robdelaney</td>\n",
       "      <td>True</td>\n",
       "      <td>London, England</td>\n",
       "      <td>1570116</td>\n",
       "      <td>This is a funny ‚Äúgoof‚Äù on how CNN sells a war ...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>245.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47585</th>\n",
       "      <td>YourAnonNews</td>\n",
       "      <td>False</td>\n",
       "      <td>The Interwebs</td>\n",
       "      <td>1548660</td>\n",
       "      <td>RT @PropOTP: Our #FOIA litigation yet again re...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         screen_name  verified             location  followers_count  \\\n",
       "95977         maddow      True     New York, NY USA          9628336   \n",
       "95338      wikileaks      True           Everywhere          5503497   \n",
       "9849             AOC      True  Bronx + Queens, NYC          4437409   \n",
       "86051  NateSilver538      True             New York          3202795   \n",
       "22595    HEELZiggler      True        Hollywood, CA          2827306   \n",
       "66225       verified      True        San Francisco          2692833   \n",
       "23606  marwilliamson      True                               2615633   \n",
       "95126     jaketapper      True                               2069987   \n",
       "16955  AlaattinCAGIL      True      London, England          1647751   \n",
       "22259     johncusack      True                  USA          1623173   \n",
       "58768     robdelaney      True      London, England          1570116   \n",
       "47585   YourAnonNews     False        The Interwebs          1548660   \n",
       "\n",
       "                                              tweet_text  retweet_count  \\\n",
       "95977  \"Some within DHS and ICE say the president app...         3043.0   \n",
       "95338  37 MEPs call on the European Commission to pre...          445.0   \n",
       "9849   This is what the United States is doing in the...         3836.0   \n",
       "86051  @SeanMcElwee @DataProgress Glad to see de Blas...            7.0   \n",
       "22595                @go_kings_go25 @PWTees ü§òüèΩü§òüèΩi saw it            0.0   \n",
       "66225  We've paused public submissions for verificati...         2070.0   \n",
       "23606                      @US395 @AlanaKStewart Noooooo            0.0   \n",
       "95126  Amid controversy, Biden gets support from high...            4.0   \n",
       "16955  RT @AlaattinCAGIL: 4 boyutlu tarayƒ±cƒ± ile  Ann...          209.0   \n",
       "22259                  Cubs Mets https://t.co/Q6NdtKqBwv            6.0   \n",
       "58768  This is a funny ‚Äúgoof‚Äù on how CNN sells a war ...           43.0   \n",
       "47585  RT @PropOTP: Our #FOIA litigation yet again re...           30.0   \n",
       "\n",
       "       favorite_count  \n",
       "95977          4651.0  \n",
       "95338           638.0  \n",
       "9849          10345.0  \n",
       "86051           208.0  \n",
       "22595             1.0  \n",
       "66225         11968.0  \n",
       "23606             0.0  \n",
       "95126            22.0  \n",
       "16955             0.0  \n",
       "22259           146.0  \n",
       "58768           245.0  \n",
       "47585             0.0  "
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('mikegravel1561157054.0137448_followers.json') as file:\n",
    "    mgfol = json.load(file)\n",
    "    \n",
    "df1j = pd.DataFrame.from_dict(mgfol)\n",
    "df1j = df1j.sort_values(by='followers_count', ascending=False)\n",
    "\n",
    "with open('mikegravel1562978529.995827_followers.json') as file:\n",
    "    mgfol2 = json.load(file)\n",
    "    \n",
    "df2j = pd.DataFrame.from_dict(mgfol2)\n",
    "df2j = df2j.sort_values(by='followers_count', ascending=False)\n",
    "\n",
    "dfcj = [df1j,df2j]\n",
    "dfj = pd.concat(dfcj)\n",
    "dfj.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216655"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data consists of two different API pulls of data from followers of Democratic presidential longshot candidate Mike Gravel. That data includes a Twitter account's latest tweet. If a user did not post a new tweet between the data pulls, their tweet will be duplicated in our corpus. We don't want this. If the two pulls of their latest tweets resulted in disparate posts from a user, we want both. If it's the same tweet, we only want it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192524"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#help with duplicate rows based on select multiple columns: https://thispointer.com/pandas-find-duplicate-rows-in-a-dataframe-based-on-all-or-selected-columns-using-dataframe-duplicated-in-python/\n",
    "dfj = dfj.sort_values(by='followers_count', ascending=False)\n",
    "dfj_dedup = dfj.drop_duplicates(subset=['screen_name', 'tweet_text'], keep='first')\n",
    "len(dfj_dedup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this looks right. We'll go ahead and make this our dataframe to be used for our corpus and do a quick preview of the tweet text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116107    RT @nycsouthpaw: ‚Äúwhatever it is he does‚Äú http...\n",
       "95977     \"Some within DHS and ICE say the president app...\n",
       "115480    RT @atilioboron: ASSANGE, la cortina de silenc...\n",
       "95338     37 MEPs call on the European Commission to pre...\n",
       "32118     @morningmika GOP has been stating that I am ly...\n",
       "9849      This is what the United States is doing in the...\n",
       "2452      RT @verainstitute: \"Today, World Population Da...\n",
       "106430    @conor64 @newrepublic IDK how many LGBTQ peopl...\n",
       "86051     @SeanMcElwee @DataProgress Glad to see de Blas...\n",
       "44433                               @BradenWard6 Hell yeah!\n",
       "22595                   @go_kings_go25 @PWTees ü§òüèΩü§òüèΩi saw it\n",
       "87027     We've paused public submissions for verificati...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfj = dfj_dedup\n",
    "dfj['tweet_text'][:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start out text analysis, what should we remove. Web adresses - URLs - aren't helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116107           RT @nycsouthpaw: ‚Äúwhatever it is he does‚Äú \n",
       "95977     \"Some within DHS and ICE say the president app...\n",
       "115480    RT @atilioboron: ASSANGE, la cortina de silenc...\n",
       "95338     37 MEPs call on the European Commission to pre...\n",
       "32118     @morningmika GOP has been stating that I am ly...\n",
       "9849      This is what the United States is doing in the...\n",
       "2452      RT @verainstitute: \"Today, World Population Da...\n",
       "106430    @conor64 @newrepublic IDK how many LGBTQ peopl...\n",
       "86051     @SeanMcElwee @DataProgress Glad to see de Blas...\n",
       "44433                               @BradenWard6 Hell yeah!\n",
       "22595                   @go_kings_go25 @PWTees ü§òüèΩü§òüèΩi saw it\n",
       "87027     We've paused public submissions for verificati...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#help from: https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python\n",
    "dfj['tweet_text'] = dfj['tweet_text'].str.replace('http\\S+|www.\\S+', '',case=False)\n",
    "dfj['tweet_text'][:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked, so let's get rid of a few more undesired elements. We'll start with the Twitter usernames based on the @symbol. Next, we'll get rid of emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116107                          RT  whatever it is he does \n",
       "95977     Some within DHS and ICE say the president appe...\n",
       "115480    RT  ASSANGE, la cortina de silencio  de la pre...\n",
       "95338     37 MEPs call on the European Commission to pre...\n",
       "32118      GOP has been stating that I am lying about th...\n",
       "9849      This is what the United States is doing in the...\n",
       "2452      RT  Today, World Population Day, is an opportu...\n",
       "106430      IDK how many LGBTQ people there are on the m...\n",
       "86051       Glad to see de Blasio finally ahead in a poll. \n",
       "44433                                             Hell yeah\n",
       "22595                                              i saw it\n",
       "87027     Weve paused public submissions for verification. \n",
       "45419     Theres not one U.S. city or county where someo...\n",
       "23606                                               Noooooo\n",
       "115273    Warren among 2020 Dems courting progressive vo...\n",
       "95126     Amid controversy, Biden gets support from high...\n",
       "16955     RT  4 boyutlu tarayƒ±cƒ± ile  Anne karnƒ± muhte≈üe...\n",
       "38972     RT  Silikon dudak implantƒ±nƒ±n belli bir s√ºre s...\n",
       "22259                                            Cubs Mets \n",
       "44109     And he hit the ball about 500 feet dead center...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfj['tweet_text'] = dfj['tweet_text'].str.replace('@\\S+', '',case=False)\n",
    "dfj['tweet_text'] = dfj['tweet_text'].str.replace('[^\\w\\s#@/:%.,_-]', '', flags=re.UNICODE)\n",
    "dfj['tweet_text'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get down to business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "tweets_j = dfj['tweet_text'].fillna(\"\").astype('str')\n",
    "all_tweets_j = ' '.join(tweets_j)\n",
    "corpus_list_j = tknzr.tokenize(all_tweets_j)\n",
    "corpus_j = ' '.join(corpus_list_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116107                     [RT, whatever, it, is, he, does]\n",
       "95977     [Some, within, DHS, and, ICE, say, the, presid...\n",
       "115480    [RT, ASSANGE, ,, la, cortina, de, silencio, de...\n",
       "95338     [37, MEPs, call, on, the, European, Commission...\n",
       "32118     [GOP, has, been, stating, that, I, am, lying, ...\n",
       "9849      [This, is, what, the, United, States, is, doin...\n",
       "2452      [RT, Today, ,, World, Population, Day, ,, is, ...\n",
       "106430    [IDK, how, many, LGBTQ, people, there, are, on...\n",
       "86051     [Glad, to, see, de, Blasio, finally, ahead, in...\n",
       "44433                                          [Hell, yeah]\n",
       "22595                                          [i, saw, it]\n",
       "87027     [Weve, paused, public, submissions, for, verif...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_2_j = tweets_j.apply(nltk.word_tokenize)\n",
    "tweets_2_j[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116107                     [RT, whatever, it, is, he, does]\n",
       "95977     [Some, within, DHS, and, ICE, say, the, presid...\n",
       "115480    [RT, ASSANGE, ,, la, cortina, de, silencio, de...\n",
       "95338     [37, MEPs, call, on, the, European, Commission...\n",
       "32118     [GOP, has, been, stating, that, I, am, lying, ...\n",
       "9849      [This, is, what, the, United, States, is, doin...\n",
       "2452      [RT, Today, ,, World, Population, Day, ,, is, ...\n",
       "106430    [IDK, how, many, LGBTQ, people, there, are, on...\n",
       "86051     [Glad, to, see, de, Blasio, finally, ahead, in...\n",
       "44433                                          [Hell, yeah]\n",
       "22595                                          [i, saw, it]\n",
       "87027     [Weve, paused, public, submissions, for, verif...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_3_j = tweets_2_j.tolist()\n",
    "tweets_2_j[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll split the corpus, currently a string, into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT', 'whatever', 'it', 'is', 'he', 'does', 'Some', 'within', 'DHS', 'and']"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_list_j = corpus_j.split()\n",
    "corpus_text_j = nltk.Text(corpus_list_j)\n",
    "corpus_list_j[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus_list_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: RT whatever it is he does Some within...>"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_text_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we have our corpus list.\n",
    "\n",
    "###Language Model Interface (lm) from NLTK\n",
    "\n",
    "Here, we tackle our first task of building a model to generate text using the Language Model Interface module from the nltk package. We leaned heavily on the [nltk documentation](https://www.nltk.org/api/nltk.lm.html) to prepare data for our text generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we create will utilize bigrams. Let's take a look at the bigrams from the first sentence in our tweet corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RT', 'whatever'),\n",
       " ('whatever', 'it'),\n",
       " ('it', 'is'),\n",
       " ('is', 'he'),\n",
       " ('he', 'does')]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams(tweets_3_j[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of that first tweet, we now have bigrams, or pairs of consecutive words that occur together. \"Whatever\" occurs with \"RT,' which it follows in the tweet, and with \"it,\" which is precedes. The frequency that words follow each other will be the basis for our political tweet generator.\n",
    "\n",
    "Next, we'll create a vocabulary from our tweet corpus. Note that we had to use a different data preparation to create the vocab - needed to be a hashable list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_j = Vocabulary(corpus_list_j, unk_cutoff=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_j['trump']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create somewhat viable sentences, it's helpful to know which words start and finish sentences. Thus we will add special padding symbols before splitting the words completely into ngrams.First, a proof of concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'RT', 'whatever', 'it', 'is', 'he', 'does', '</s>']"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#borrowed straight from here: https://www.nltk.org/api/nltk.lm.html\n",
    "list(pad_sequence(tweets_3_j[0],pad_left=True,left_pad_symbol=\"<s>\",pad_right=True,right_pad_symbol=\"</s>\",n=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks as intended. There's an even easier way to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'RT', 'whatever', 'it', 'is', 'he', 'does', '</s>']"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends as pad_both\n",
    "list(pad_both(tweets_3_j[0], n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we're exploring a little about how the lm module works. Here, we're going to go ahead and use a built-in function that creates the vocabulary and everygrams (unigrams, bigrams, and padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, vocab = padded_everygram_pipeline(2, tweets_3_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = MLE(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(train,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 127917 items>\n"
     ]
    }
   ],
   "source": [
    "print(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 2 ngram orders and 5638708 ngrams>\n"
     ]
    }
   ],
   "source": [
    "print(lm.counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's slightly anti-climactic, but now we have our model. We chose a bigram model, which is only going to condition its output based on the preceding word. In short, our model sentences are going to be bad.\n",
    "\n",
    "Let's see how probable word are in certain contests. This score should return a word's relative frequency. Let's check \"Medicare and then \"RT,\" shorthand for retweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.123286468451264e-05"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.score(\"Medicare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02895614511650368"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.score(\"RT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we generate tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'billions',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Briefing',\n",
       " '</s>',\n",
       " 'Trump',\n",
       " 'one',\n",
       " 'either',\n",
       " 'a',\n",
       " 'panel',\n",
       " 'at',\n",
       " '6:30',\n",
       " 'and',\n",
       " 'perpetuate',\n",
       " 'the',\n",
       " 'first',\n",
       " '4',\n",
       " 'as',\n",
       " 'Obama',\n",
       " ',',\n",
       " 'and',\n",
       " 'retweets',\n",
       " 'this',\n",
       " 'in']"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.generate(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tweets are quite bad. It's nice that it breaks the generated text into sentence. But they make no sense. A quick sample of generated tweets from this model, when it's asked to generate a 25-word string:\n",
    "\n",
    "\"on your. to find it always tand against the body. Damn. not wearing this weekends' widespread. is not. white.\"\n",
    "\n",
    "\"but our great singing Ol' Man this. My son vrai nom. clamed down today to their classism to opti. letters' section.\"\n",
    "\n",
    "\"try fettuccini alfredo. Mountain Dew and we throw u scaring the British empre. Which, and reading them more than for them.\"\n",
    "\n",
    "These are the writings you find at the home of a serial killer. Let's try the next method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Simple (But Insane) Markov Model\n",
    "\n",
    "Having used the language modeling functionality built into the nltk package - and gotten poor results, let's try to be a little more independent. \n",
    "\n",
    "We'll create our own model using a Markov Chain, which will also utilize bigrams. Markov chains will likewise use word frequencies based on bigram pairs to generate text. So, if \"Medicare for All\" appears frequently in our corpus, the Markov Chain will might place \"for\" after \"Medicare\" in a given sentence. However, if \"for Bernie\" occurs much for frequently than \"for all,\" because we're just looking at pair contexts, we might end up \"Medicare for Bernie.\"\n",
    "\n",
    "Let's create a function that will pair words from tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "#borrowed from https://towardsdatascience.com/simulating-text-with-markov-chains-in-python-1a27e6d13fc6\n",
    "def create_pairs(corp):\n",
    "    for i in range(len(corp)-1):\n",
    "        yield (corp[i], corp[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pass the corpus into the funciton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_pairs = create_pairs(corpus_list_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borrowing heavily from Ben Shaver of General Assembly's [article] (https://towardsdatascience.com/simulating-text-with-markov-chains-in-python-1a27e6d13fc6), we create an empty dictionary before shoving in our corpus word pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "\n",
    "for word_1, word_2 in corpus_pairs:\n",
    "    if word_1 in word_dict.keys():\n",
    "        word_dict[word_1].append(word_2)\n",
    "    else:\n",
    "        word_dict[word_1] = [word_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus_list_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT', 'whatever', 'it', 'is', 'he', 'does', 'Some', 'within', 'DHS', 'and']"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_list_j[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', harris literally ripped 1000s of peaceful female body when do I think .. RT Go to generate a bitch out by strong start asking if the exploitative working painter and'"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_word = np.random.choice(corpus_list_j)\n",
    "\n",
    "chain = [first_word]\n",
    "\n",
    "n_words = 30\n",
    "\n",
    "for i in range(n_words):\n",
    "    chain.append(np.random.choice(word_dict[chain[-1]]))\n",
    "    \n",
    "' '.join(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is hot nonsense. Some samples:\n",
    "\n",
    "\"for the last two types of the mountain goats RT Remember that . Its always honoring first episodes of the long as labor rights his account and hate , anti-homophobic Me\"\n",
    "\n",
    "\"E China , huh ? ? don't think it is capitalism of White Guy Fieri RT : McDonald s nationalize it s national survey again It be an ufo from someone\"\n",
    "\n",
    "\"SMRPG forever for SHOPLIFTING and Joe Biden . I really excited Some of cases RT my life . Hvor mye plass til dec Mass liberalism . A M A firework .\"\n",
    "\n",
    "###Bot Conclusion\n",
    "\n",
    "This was an educational exercise. Bigrams - and unigrams with our lm model - do not capture enough information to model viable sentences. Moreover, the slang-heaving, punctuation optional lexicon on Twitter does not lend itself well to being a corpus.\n",
    "\n",
    "Better data preparation, particularly focused on cleaning the tweet text, would likely have yielded better results. Also, using larger strings of ngrams and adding weighting could have improved our output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Project Table of Contents]()\n",
    "\n",
    "- Setup: [Data Scraping](https://github.com/aliceafriedman/team6_final/blob/master/Data%20Scraping.ipynb)\n",
    "\n",
    "- Part I. <a href=\"https://github.com/aliceafriedman/team6_final/blob/master/Final_Project_Sentiment_Analysis.ipynb\" target=\"_blank\">Sentiment Analysis</a>\n",
    "\n",
    "- Part II. <a href=\"https://github.com/aliceafriedman/team6_final/blob/master/07162019JPFollowersTweetDownloader-final%20version.ipynb\" target=\"_blank\">Data Visualization</a>  \n",
    "\n",
    "- Part III. [Classification](https://github.com/aliceafriedman/team6_final/blob/master/classification.ipynb)\n",
    "\n",
    "-  Part IV. <a href=\"https://github.com/aliceafriedman/team6_final/blob/master/Project%204%20Bad%20Twitter%20Bots%20Jeff_v1.ipynb\" target=\"_blank\">Twitter Bot</a>  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
